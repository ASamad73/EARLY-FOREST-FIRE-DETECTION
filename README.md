Our project began with establishing a baseline using CLIP's zero-shot classification capabilities. We tested three prompt strategies of varying complexity across multiple datasets. Simple prompts achieved only 35% accuracy on augmented data and 23% on original images, demonstrating their ineffectiveness. More detailed prompts showed significant improvement, reaching 80-88% accuracy, confirming that richer contextual prompts yield better results. Interestingly, non-augmented data consistently outperformed augmented versions in our zero-shot inference tests, suggesting that augmentation techniques may introduce noise when used during inference rather than training.

The first major improvement involved developing a two-stage pipeline combining YOLOv8 and CLIP. Initial tests revealed YOLOv8 performed better on general fire images compared to distant forest fire scenes, which presented greater detection challenges due to subtle visual cues. Following professor recommendations, we enhanced the system by integrating YOLOv8 and CLIP for joint inferences. This integrated approach showed notable performance improvements, particularly for the challenging forest fire dataset.

For our second improvement, we implemented several advanced techniques across different implementations. The classifier version used YOLOv8 for localization and CLIP for verification, balanced through a logistic classifier with weights alpha=0.6 and beta=0.4. The padding variation retained more contextual information by applying 10% padding during classifier training. We also experimented with simplified prompts of the same categories but shorter length, and tested different weight configurations where CLIP's influence (beta) was increased relative to YOLO's (alpha). These refinements allowed us to optimize the balance between precise localization and semantic understanding for improved fire detection performance.

It is to note that each folder has its own seperate readme file as well that explains that part.
